**(100,3)Ïùò X_data LR(MVLR)**


```python
import torch
import torch.nn
import torch.nn.functional as F
import torch.optim as optim
```

### 1. Trainable variable


```python
x_train = torch.FloatTensor(np.random.normal(size=(100,3)))
y_train = 5*x_train.sum(axis=1)*(1/3) + 5
```

### 2. parameter

- w = (3,1)
- b = (100,1)


```python
w = torch.zeros((3,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True)
```

### 3. hypothesis


```python
h = x_train.matmul(w) + b
```

### 4. model


```python
epochs = 100
optimizer = optim.SGD([w,b],lr=1e-5)
y_true = y_train

for epoch in range(epochs+1):
    
    y_pred = (x_train.matmul(w) + b).squeeze()
    cost = torch.mean((y_pred-y_true)**2)
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch%10 == 0:
        print(f"cost = {cost.item()}, epoch = {epoch}, w = {torch.mean(w)}")
        
```

    cost = 30.02829360961914, epoch = 0, w = 2.6630919819581322e-05
    cost = 30.017255783081055, epoch = 10, w = 0.00029291573446244
    cost = 30.006223678588867, epoch = 20, w = 0.0005591563531197608
    cost = 29.995197296142578, epoch = 30, w = 0.0008253526757471263
    cost = 29.984176635742188, epoch = 40, w = 0.0010915049351751804
    cost = 29.97315216064453, epoch = 50, w = 0.0013576127821579576
    cost = 29.962139129638672, epoch = 60, w = 0.001623676624149084
    cost = 29.951122283935547, epoch = 70, w = 0.00188969646114856
    cost = 29.940122604370117, epoch = 80, w = 0.0021556715946644545
    cost = 29.929119110107422, epoch = 90, w = 0.0024216030724346638
    cost = 29.918119430541992, epoch = 100, w = 0.002687490312382579

