```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
```




    <torch._C.Generator at 0x7febe334cd90>



### 1. forward


```python
x = torch.rand(size=(3,5),requires_grad=True)
x
```




    tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],
            [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],
            [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]], requires_grad=True)




```python
y_pred = F.softmax(x,dim=1)
y_pred
```




    tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],
            [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],
            [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)



### 2. true_labeling(one-hot)


```python
y = torch.FloatTensor([0,2,1]).long()
```


```python
y.view(-1,1)
```




    tensor([[0],
            [2],
            [1]])




```python
y_onehot = torch.zeros_like(x)
y_onehot
```




    tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])




```python
# scatter -> 대상 엘리먼트들이 long 타입이어야 함(long -> integer 계열)
y_onehot = y_onehot.scatter(1,y.view(-1,1),1)
y_onehot
```




    tensor([[1., 0., 0., 0., 0.],
            [0., 0., 1., 0., 0.],
            [0., 1., 0., 0., 0.]])




```python
y_pred
```




    tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],
            [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],
            [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)



### 3. Loss function (Cross Entorpy)


```python
cost = (y_onehot * -torch.log(y_pred)).sum(dim=1).mean()
cost
```




    tensor(1.4689, grad_fn=<MeanBackward0>)



### 4. F.cross_entropy


```python
cost = F.cross_entropy(x,y)
cost
```




    tensor(1.4689, grad_fn=<NllLossBackward>)




```python
F.log_softmax(x,dim=1)
```




    tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],
            [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],
            [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],
           grad_fn=<LogSoftmaxBackward>)




```python
F.nll_loss(F.log_softmax(x,dim=1),y)
```




    tensor(1.4689, grad_fn=<NllLossBackward>)


