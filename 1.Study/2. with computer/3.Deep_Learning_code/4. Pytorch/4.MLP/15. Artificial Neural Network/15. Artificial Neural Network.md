# 1. MLP 구현


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
```


```python
device = "cuda" if torch.cuda.is_available() else "cpu"

torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
```


```python
x = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)
y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)
```


```python
model = nn.Sequential(nn.Linear(2,10, bias=True),nn.Sigmoid(),
                      nn.Linear(10,10, bias=True),nn.Sigmoid(),
                      nn.Linear(10,10, bias=True),nn.Sigmoid(),
                      nn.Linear(10,1, bias=True),nn.Sigmoid())
```


```python
# optimizer
optimizer = optim.SGD(model.parameters(),lr=1)
```


```python
nb_epochs = 10000
loss_list = []

for epoch in range(nb_epochs+1):
    
    # forward
    pred = model(x)
    cost = F.binary_cross_entropy(pred,y)
    
    # backward
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch % 1000 == 0:
        print("epoch : {}, cost : {:.4f}".format(epoch, cost.item()))
    
```

    epoch : 0, cost : 0.6949
    epoch : 1000, cost : 0.6931
    epoch : 2000, cost : 0.6931
    epoch : 3000, cost : 0.6931
    epoch : 4000, cost : 0.6929
    epoch : 5000, cost : 0.6821
    epoch : 6000, cost : 0.0013
    epoch : 7000, cost : 0.0005
    epoch : 8000, cost : 0.0003
    epoch : 9000, cost : 0.0002
    epoch : 10000, cost : 0.0002


# Test


```python
with torch.no_grad():
    pred = model(x)
    predicted = (pred > 0.5).float()
    accuracy = (predicted == y).float().mean()
    print(f"정확도 : {accuracy}")
```

    정확도 : 1.0

