이 노트북은 다음에 대한 실험 결과가 실려 있습니다.

* 손실함수
* 학습률 감쇠
* 가중치 초기화
* 최적화 기법
* 드롭아웃

# `lincoln` 라이프러리 임포트하기


```python
import sys
# 예제 파일 경로로 수정한 다음 주석 해제
sys.path.append(r'/Users/kimjeongseob/Desktop/Study/1.Study/2. with computer/3.Deep_Learning_code/5. Deep Learning from Scratch')
```


```python
import numpy as np
```


```python
%load_ext autoreload
%autoreload 2
```


```python
import lincoln
from lincoln.layers import Dense
from lincoln.losses import SoftmaxCrossEntropy, MeanSquaredError
from lincoln.optimizers import Optimizer, SGD, SGDMomentum
from lincoln.activations import Sigmoid, Tanh, Linear, ReLU
from lincoln.network import NeuralNetwork
from lincoln.train import Trainer
from lincoln.utils import mnist
from lincoln.utils.np_utils import softmax
```


```python
mnist.init() # 최초 실행시 주석 해제, 이후 다시 주석 처리할 것
```

    Downloading train-images-idx3-ubyte.gz...
    Downloading t10k-images-idx3-ubyte.gz...
    Downloading train-labels-idx1-ubyte.gz...
    Downloading t10k-labels-idx1-ubyte.gz...
    Download complete.
    Save complete.



```python
X_train, y_train, X_test, y_test = mnist.load()
```


```python
num_labels = len(y_train)
num_labels
```




    60000




```python
# 원-핫 인코딩
num_labels = len(y_train)
train_labels = np.zeros((num_labels, 10))
for i in range(num_labels):
    train_labels[i][y_train[i]] = 1

num_labels = len(y_test)
test_labels = np.zeros((num_labels, 10))
for i in range(num_labels):
    test_labels[i][y_test[i]] = 1
```

# MNIST 데모

# 데이터를 평균 0 분산 1로 정규화


```python
X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)
```


```python
np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)
```




    (-33.318421449829934,
     221.68157855017006,
     -33.318421449829934,
     221.68157855017006)




```python
X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)
```


```python
np.min(X_train), np.max(X_train), np.min(X_test), np.max(X_test)
```




    (-0.424073894391566, 2.821543345689335, -0.424073894391566, 2.821543345689335)




```python
def calc_accuracy_model(model, test_set):
    return print(f'''모델 검증을 위한 정확도: {np.equal(np.argmax(model.forward(test_set, inference=True), axis=1), y_test).sum() * 100.0 / test_set.shape[0]:.2f}%''')
```

## 소프트맥스 교차 엔트로피

### sigmoid 활성화 함수를 사용한 경우


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh()),
            Dense(neurons=10, 
                  activation=Sigmoid())],
            loss = MeanSquaredError(normalize=False), 
seed=20190119)

trainer = Trainer(model, SGD(0.1))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);
print()
calc_accuracy_model(model, X_test)
```

    10에폭에서 검증 데이터에 대한 손실값: 0.611
    20에폭에서 검증 데이터에 대한 손실값: 0.428
    30에폭에서 검증 데이터에 대한 손실값: 0.389



    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-14-d7df4b843a02> in <module>
         12             eval_every = 10,
         13             seed=20190119,
    ---> 14             batch_size=60);
         15 print()
         16 calc_accuracy_model(model, X_test)


    ~/Desktop/Study/1.Study/2. with computer/3.Deep_Learning_code/5. Deep Learning from Scratch/lincoln/train.py in fit(self, X_train, y_train, X_test, y_test, epochs, eval_every, batch_size, seed, single_output, restart, early_stopping, conv_testing)
         80             if (e+1) % eval_every == 0:
         81 
    ---> 82                 test_preds = self.net.forward(X_test, inference=True)
         83                 loss = self.net.loss.forward(test_preds, y_test)
         84 


    ~/Desktop/Study/1.Study/2. with computer/3.Deep_Learning_code/5. Deep Learning from Scratch/lincoln/network.py in forward(self, X_batch, inference)
         18         X_out = X_batch
         19         for layer in self.layers:
    ---> 20             X_out = layer.forward(X_out, inference)
         21 
         22         return X_out


    ~/Desktop/Study/1.Study/2. with computer/3.Deep_Learning_code/5. Deep Learning from Scratch/lincoln/layers.py in forward(self, input_, inference)
         37         for operation in self.operations:
         38 
    ---> 39             input_ = operation.forward(input_, inference)
         40 
         41         self.output = input_


    ~/Desktop/Study/1.Study/2. with computer/3.Deep_Learning_code/5. Deep Learning from Scratch/lincoln/base.py in forward(self, input_, inference)
         15         self.input_ = input_
         16 
    ---> 17         self.output = self._output(inference)
         18 
         19         return self.output


    ~/Desktop/Study/1.Study/2. with computer/3.Deep_Learning_code/5. Deep Learning from Scratch/lincoln/activations.py in _output(self, inference)
         42 
         43     def _output(self, inference: bool) -> ndarray:
    ---> 44         return np.tanh(self.input_)
         45 
         46     def _input_grad(self, output_grad: ndarray) -> ndarray:


    KeyboardInterrupt: 


메모: 분류 모델의 출력을 평균제곱오차 손실로 정규화 하더라도 성능이 향상되지 않는다.


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh()),
            Dense(neurons=10, 
                  activation=Sigmoid())],
            loss = MeanSquaredError(normalize=True), 
seed=20190119)

trainer = Trainer(model, SGD(0.1))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);

calc_accuracy_model(model, X_test)
```

소프트맥스 교차 엔트로피 손실함수를 사용해야 하는 이유가 여기에 있다.

#### sigmoid 활성화 함수를 사용한 경우


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Sigmoid()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGD(0.1))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 130,
            eval_every = 1,
            seed=20190119,
            batch_size=60);
print()
calc_accuracy_model(model, X_test)
```

####  ReLU 활성화 함수를 사용한 경우


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=ReLU()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGD(0.1))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);
print()
calc_accuracy_model(model, X_test)
```


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGD(0.1))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);
print()
calc_accuracy_model(model, X_test)
```

## 모멘텀을 적용한 확률적 경사 하강법


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Sigmoid()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

optim = SGDMomentum(0.1, momentum=0.9)

trainer = Trainer(model, SGDMomentum(0.1, momentum=0.9))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 1,
            seed=20190119,
            batch_size=60);

calc_accuracy_model(model, X_test)
```


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

optim = SGD(0.1)

optim = SGDMomentum(0.1, momentum=0.9)

trainer = Trainer(model, SGDMomentum(0.1, momentum=0.9))
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);

calc_accuracy_model(model, X_test)
```

## 여러 가지 학습률 감쇠 기법


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

optimizer = SGDMomentum(0.15, momentum=0.9, final_lr = 0.05, decay_type='linear')

trainer = Trainer(model, optimizer)
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);

calc_accuracy_model(model, X_test)
```


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh()),
            Dense(neurons=10, 
                  activation=Linear())],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

optimizer = SGDMomentum(0.2, 
                        momentum=0.9, 
                        final_lr = 0.05, 
                        decay_type='exponential')

trainer = Trainer(model, optimizer)
trainer.fit(X_train, train_labels, X_test, test_labels,
            epochs = 50,
            eval_every = 10,
            seed=20190119,
            batch_size=60);

calc_accuracy_model(model, X_test)
```

## 가중치 초기화 방법에 대한 실험


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh(),
                  weight_init="glorot"),
            Dense(neurons=10, 
                  activation=Linear(),
                  weight_init="glorot")],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

optimizer = SGDMomentum(0.15, momentum=0.9, final_lr = 0.05, decay_type='linear')

trainer = Trainer(model, optimizer)
trainer.fit(X_train, train_labels, X_test, test_labels,
       epochs = 50,
       eval_every = 10,
       seed=20190119,
           batch_size=60,
           early_stopping=True);

calc_accuracy_model(model, X_test)
```


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh(),
                  weight_init="glorot"),
            Dense(neurons=10, 
                  activation=Linear(),
                  weight_init="glorot")],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))
trainer.fit(X_train, train_labels, X_test, test_labels,
       epochs = 50,
       eval_every = 10,
       seed=20190119,
           batch_size=60,
           early_stopping=True);

calc_accuracy_model(model, X_test)
```

## 드롭아웃


```python
model = NeuralNetwork(
    layers=[Dense(neurons=89, 
                  activation=Tanh(),
                  weight_init="glorot",
                  dropout=0.8),
            Dense(neurons=10, 
                  activation=Linear(),
                  weight_init="glorot")],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))
trainer.fit(X_train, train_labels, X_test, test_labels,
       epochs = 50,
       eval_every = 10,
       seed=20190119,
           batch_size=60,
           early_stopping=True);

calc_accuracy_model(model, X_test)
```

## 드롭아웃 적용 여부에 따른 딥러닝 성능 차이


```python
model = NeuralNetwork(
    layers=[Dense(neurons=178, 
                  activation=Tanh(),
                  weight_init="glorot",
                  dropout=0.8),
            Dense(neurons=46, 
                  activation=Tanh(),
                  weight_init="glorot",
                  dropout=0.8),
            Dense(neurons=10, 
                  activation=Linear(),
                  weight_init="glorot")],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))
trainer.fit(X_train, train_labels, X_test, test_labels,
       epochs = 100,
       eval_every = 10,
       seed=20190119,
           batch_size=60,
           early_stopping=True);

calc_accuracy_model(model, X_test)
```


```python
model = NeuralNetwork(
    layers=[Dense(neurons=178, 
                  activation=Tanh(),
                  weight_init="glorot"),
            Dense(neurons=46, 
                  activation=Tanh(),
                  weight_init="glorot"),
            Dense(neurons=10, 
                  activation=Linear(),
                  weight_init="glorot")],
            loss = SoftmaxCrossEntropy(), 
seed=20190119)

trainer = Trainer(model, SGDMomentum(0.2, momentum=0.9, final_lr = 0.05, decay_type='exponential'))
trainer.fit(X_train, train_labels, X_test, test_labels,
       epochs = 100,
       eval_every = 10,
       seed=20190119,
           batch_size=60,
           early_stopping=True);

calc_accuracy_model(model, X_test)
```


```python

```
