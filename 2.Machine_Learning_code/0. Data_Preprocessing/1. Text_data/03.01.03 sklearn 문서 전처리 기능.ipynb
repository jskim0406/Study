{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03.01.03 sklearn 문서 전처리 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BOW (Bag of Words)\n",
    "\n",
    "    - corpus : 전체 문서 (d1,d2,d3, ...)\n",
    "    - corpus 를 구성하는 고정된 단어장 생성 (단어장 내 단어 : t1, t2, t3, ...)\n",
    "    \n",
    "$X_{i,j}$ = 문서 $d_{i}$ 내 단어 $t_{j}$의 출현 빈도\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. sklearn 문서 전처리 패키지 및 클래스\n",
    "\n",
    "```\n",
    "\n",
    "Scikit-Learn의 feature_extraction 서브패키지와 feature_extraction.text 서브패키지는 다음과 같은 문서 전처리용 클래스를 제공한다.\n",
    "\n",
    "1. DictVectorizer:\n",
    "각 단어의 수를 세어놓은 사전에서 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "\n",
    "2. CountVectorizer:\n",
    "문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "*단어장 만들어놓고, count해서 BOW 만드는 방식은 단어장이 커지면 실제로 사용하기 어렵다는 단점 => HashingVectorizer 사용\n",
    "\n",
    "\n",
    "3. TfidfVectorizer:\n",
    "CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "*CountVectorizer는 단어장에 있는 숫자를 세주기만 함 (빈도수)\n",
    "*TfidVectorizer는 빈도수 + 중요도에 따라 그 숫자를 줄이거나 늘리는 역할\n",
    "\n",
    "\n",
    "4. HashingVectorizer:\n",
    "해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "*실무에서 우리가 쓰는 단어장이 너무 클 때, 메모리등의 문제로 처리가 힘든 경우가 있음. 이럴 때 사용\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1) CountVectorizer\n",
    "\n",
    "    1. 문서를 토큰 리스트로 변환\n",
    "    2. 각 문서에서 토큰의 출현 빈도 count\n",
    "    3. 각 문서를 BOW 인코딩 벡터로 변환\n",
    "    \n",
    "    \n",
    "    <클래스 사용법>\n",
    "    1. 클래스 객체 생성\n",
    "    2. 말뭉치 넣고 fit 메서드 실행\n",
    "    3. vocabulary_ 속성에 단어장이 자동 생성됨\n",
    "    4. transform 메서드로 다른 문서를 BOW 인코딩\n",
    "    5. BOW 인코딩 된 결과는 Sparse 행렬로 만들어지므로, toarray메서드로 보통 행렬로 변환\n",
    "        *sparse 행렬 : 0이 아닌 데이터만 행렬로 만드는 것 (메모리 효율화 차원)\n",
    "        *toarray 메서드 : 우리가 아는 보통의 numpy array 로 바뀜\n",
    "    \n",
    "    <클래스 사용 code>\n",
    "    0. corpus = ['AAA','BBB', ..]\n",
    "    1. vect = CountVectorizer()\n",
    "    2. vect.fit(corpus)\n",
    "    3. vect.vocabulary_\n",
    "    4-5. vect.transform(['AAA']).toarray()\n",
    "    \n",
    "    \n",
    "    \n",
    "    <CountVectorizer() 인수>\n",
    "    CountVectorizer는 이러한 작업을 하기 위한 다음과 같은 인수를 가질 수 있다.\n",
    "```\n",
    "\n",
    "[단어장 생성 관련]\n",
    "    stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "        stop words 목록.‘english’이면 영어용 스탑 워드 사용. stop_words는 단어장에 들어가지 않음\n",
    "    ngram_range : (min_n, max_n) 튜플,      *N그램 : 단어장 생성에 사용할 토큰의 크기\n",
    "        n-그램 범위\n",
    "    max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "        단어장에 포함되기 위한 최대 빈도\n",
    "    min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1\n",
    "        단어장에 포함되기 위한 최소 빈도\n",
    "\n",
    "[tokenizer 관련]\n",
    "    analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수\n",
    "        단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "    token_pattern : string\n",
    "        토큰 정의용 정규 표현식\n",
    "    tokenizer : 함수 또는 None (디폴트)\n",
    "        토큰 생성 함수 .\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2) TF-IDF\n",
    "\n",
    "Tf : term frequency (빈도수)\n",
    "idf : inverse Document frequency\n",
    "\n",
    "    - 단어장에서 그대로 카운트하지 않고, 중요도로 가중 !\n",
    "        *중요도 = idf = 모든 문서에 공통적으로 들어있는 단어는 중요하지 않은 것(문서 구별능력이 떨어짐) = 가중치 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3) Hashing Trick\n",
    "\n",
    "    - Hash function\n",
    "        : 문자를 받으면 숫자를 출력하는 함수 (속도가 매우 빠름)\n",
    "        : BOW로 vectorize 시, 단어장에서 찾는 대신(Count, TF-IDF) 해시함수를 이용해 속도를 높인다)\n",
    "        \n",
    "    <CountVectorizer>\n",
    "        - Count방식은 모든 작업을 메모리 상에서 수행\n",
    "        - 처리할 문서의 크기가 커지면, 단어장 딕셔너리가 커진다.\n",
    "        - 실행 속도가 느려지거나, 실행이 불가능해짐\n",
    "        \n",
    "    <HashingVectorizer>\n",
    "        - 해시함수 사용\n",
    "        - 단어에 대한 인덱스 번호를 수식으로 생성\n",
    "        - 사전 메모리 필요없고, 실행 시간 단축 가능\n",
    "        - 단어의 충돌 가능(매우 드물게)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gensim 패키지\n",
    "\n",
    "    - Bag of Words 인코딩\n",
    "    - TF - IDF 인코딩\n",
    "    - 토픽 모델링\n",
    "    \n",
    "    \n",
    "    <Gensim의 BOW 인코잉 기능>\n",
    "    \n",
    "    1. Dictionary 클래스 이용\n",
    "        - token2id 속성으로 사전 저장\n",
    "        - doc2bow 메서드로 bow 저장\n",
    "        \n",
    "    2. TfidModel 클래스를 이용하면 TF - IDF 인코딩도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1) corpus 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2) 토큰 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'the', 'first', 'document.'],\n",
       " ['This', 'is', 'the', 'second', 'second', 'document.'],\n",
       " ['And', 'the', 'third', 'one.'],\n",
       " ['Is', 'this', 'the', 'first', 'document?'],\n",
       " ['The', 'last', 'document?']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [[text for text in doc.split()] for doc in corpus]\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3) Dictionary 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 0,\n",
       " 'document.': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'the': 4,\n",
       " 'second': 5,\n",
       " 'And': 6,\n",
       " 'one.': 7,\n",
       " 'third': 8,\n",
       " 'Is': 9,\n",
       " 'document?': 10,\n",
       " 'this': 11,\n",
       " 'The': 12,\n",
       " 'last': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(token_list)\n",
    "dictionary.token2id    #sklearn 에선 dictionary.vocabulary_ 일 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4) BOW 인코딩\n",
    "\n",
    "    - sparse 행렬로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (1, 1), (3, 1), (4, 1), (5, 2)],\n",
       " [(4, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(2, 1), (4, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(10, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = [dictionary.doc2bow(token) for token in token_list]\n",
    "term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5) TF-IDF 인코딩\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc : \n",
      "0 0.49633406058198626\n",
      "1 0.49633406058198626\n",
      "2 0.49633406058198626\n",
      "3 0.49633406058198626\n",
      "4 0.12087183801361165\n",
      "doc : \n",
      "0 0.25482305694621393\n",
      "1 0.25482305694621393\n",
      "3 0.25482305694621393\n",
      "4 0.0620568558708622\n",
      "5 0.8951785160431313\n",
      "doc : \n",
      "4 0.07979258234193365\n",
      "6 0.5755093812740171\n",
      "7 0.5755093812740171\n",
      "8 0.5755093812740171\n",
      "doc : \n",
      "2 0.3485847413542797\n",
      "4 0.08489056411237639\n",
      "9 0.6122789185961829\n",
      "10 0.3485847413542797\n",
      "11 0.6122789185961829\n",
      "doc : \n",
      "10 0.37344696513776354\n",
      "12 0.6559486886294514\n",
      "13 0.6559486886294514\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(term_matrix)\n",
    "\n",
    "for doc in tfidf[term_matrix]:\n",
    "    print(\"doc : \")\n",
    "    for k, v in doc:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Topic modeling 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 텍스트 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(categories = ['comp.graphics','rec.sport.baseball','sci.med'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 명사추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tagged_list = [pos_tag(word_tokenize(doc)) for doc in newsgroups.data]\n",
    "nouns_list = [[t[0] for t in doc if t[1].startswith('N')] for doc in tagged_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "nouns_list = [[lm.lemmatize(w, pos='n') for w in doc] for doc in nouns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "token_list = [[text.lower() for text in doc] for doc in nouns_list]\n",
    "token_list = [[re.sub(r\"[^A-Za-z]+\",'',word) for word in doc] for doc in token_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Topic 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(token_list)\n",
    "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in token_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
