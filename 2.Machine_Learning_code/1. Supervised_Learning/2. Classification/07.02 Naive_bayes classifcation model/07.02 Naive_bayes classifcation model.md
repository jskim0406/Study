# 07.02 naive-bayes classification model

- conditional independence
- navie assumption
- likelihood model (ë°ì´í„°ì˜ íŠ¹ì§•ì— ë”°ë¼ ì •ê·œë¶„í¬, ë² ë¥´ëˆ„ì´, ë‹¤í•­ë¶„í¬)
    - ê¸°ì¡´ ê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ ë¶„ë¦¬ê°€ëŠ¥í•¨ìˆ˜í™” (ë‚˜ì´ë¸Œ ê°€ì •(í´ë˜ìŠ¤ ì¡°ê±´ë¶€ ë…ë¦½)ì— ë”°ë¼)

```

[ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•˜ëŠ” ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•]

ì‚¬ì´í‚·ëŸ°ì˜ naive_bayes ì„œë¸ŒíŒ¨í‚¤ì§€ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„¸ê°€ì§€ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤.

GaussianNB: ì •ê·œë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ
BernoulliNB: ë² ë¥´ëˆ„ì´ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ
MultinomialNB: ë‹¤í•­ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ

ì´ í´ë˜ìŠ¤ë“¤ì€ ë‹¤ì–‘í•œ ì†ì„±ê°’ ë° ë©”ì„œë“œë¥¼ ê°€ì§„ë‹¤. ìš°ì„  ì‚¬ì „ í™•ë¥ ê³¼ ê´€ë ¨ëœ ì†ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.


classes_
ì¢…ì†ë³€ìˆ˜ Yì˜ í´ë˜ìŠ¤(ë¼ë²¨)


class_count_
ì¢…ì†ë³€ìˆ˜ Yì˜ ê°’ì´ íŠ¹ì •í•œ í´ë˜ìŠ¤ì¸ í‘œë³¸ ë°ì´í„°ì˜ ìˆ˜


class_prior_
ì¢…ì†ë³€ìˆ˜ Yì˜ ë¬´ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬  ğ‘ƒ(ğ‘Œ)  (ì •ê·œë¶„í¬ì˜ ê²½ìš°ì—ë§Œ)


class_log_prior_
ì¢…ì†ë³€ìˆ˜ Yì˜ ë¬´ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ì˜ ë¡œê·¸  logğ‘ƒ(ğ‘Œ)  (ë² ë¥´ëˆ„ì´ë¶„í¬ë‚˜ ë‹¤í•­ë¶„í¬ì˜ ê²½ìš°ì—ë§Œ)




[ì •ê·œë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•]

ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• GaussianNBì€ ê°€ëŠ¥ë„ ì¶”ì •ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì†ì„±ì„ ê°€ì§„ë‹¤.

theta_: ì •ê·œë¶„í¬ì˜ ê¸°ëŒ“ê°’  ğœ‡ 
sigma_: ì •ê·œë¶„í¬ì˜ ë¶„ì‚°  ğœ2



[ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•]

ë² ë¥´ëˆ„ì´ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• í´ë˜ìŠ¤ BernoulliNBëŠ” ê°€ëŠ¥ë„ ì¶”ì •ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ì†ì„±ì„ ê°€ì§„ë‹¤.

feature_count_: ê° í´ë˜ìŠ¤  ğ‘˜ ì— ëŒ€í•´  ğ‘‘ ë²ˆì§¸ ë™ì „ì´ ì•ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜  ğ‘ğ‘‘,ğ‘˜ 
feature_log_prob_: ë² ë¥´ëˆ„ì´ë¶„í¬ ëª¨ìˆ˜ì˜ ë¡œê·¸

```

# 1. ì •ê·œë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•


    - ë°ì´í„° : ì‹¤ìˆ˜, í´ë˜ìŠ¤ ë§ˆë‹¤ íŠ¹ì •í•œ ê°’ ì£¼ë©´ì—ì„œ ë°œìƒí•  ê²½ìš°
    - ì •ê·œë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• : ì •ê·œë¶„í¬ ê°€ëŠ¥ë„ëª¨í˜•ì„ ì ìš©í•œ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨ë¸
        - ì •ê·œë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜• : ë‹¤ë³€ìˆ˜ ì •ê·œë¶„í¬ pdfì— ì¡°ê±´ë¶€ ë…ë¦½ ì ìš©
        

**ë³¸ë˜ ë‹¤ë³€ìˆ˜ ì •ê·œë¶„í¬ ë°ì´í„°**


```python
np.random.seed(0)
rv0 = sp.stats.multivariate_normal([-2, -2], [[1, 0.9], [0.9, 2]])
rv1 = sp.stats.multivariate_normal([2, 2], [[1.2, -0.8], [-0.8, 2]])
X0 = rv0.rvs(40)
X1 = rv1.rvs(60)
X = np.vstack([X0, X1])
y = np.hstack([np.zeros(40), np.ones(60)])

xx1 = np.linspace(-5, 5, 100)
xx2 = np.linspace(-5, 5, 100)
XX1, XX2 = np.meshgrid(xx1, xx2)
plt.grid(False)
plt.contour(XX1, XX2, rv0.pdf(np.dstack([XX1, XX2])), cmap=mpl.cm.cool)
plt.contour(XX1, XX2, rv1.pdf(np.dstack([XX1, XX2])), cmap=mpl.cm.hot)
plt.scatter(X0[:, 0], X0[:, 1], marker="o", c='b', label="y=0")
plt.scatter(X1[:, 0], X1[:, 1], marker="s", c='r', label="y=1")
plt.legend()
plt.title("ë°ì´í„°ì˜ í™•ë¥ ë¶„í¬")
plt.axis("equal")
plt.show()
```


![png](output_4_0.png)


**ë³¸ë˜ ë‹¤ë³€ìˆ˜ ì •ê·œë¶„í¬ -> ë‚˜ì´ë¸Œ ê°€ì •ì„ ì ìš©í•œ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ìœ¼ë¡œ í•´ì„**


```python
from sklearn.naive_bayes import GaussianNB
model_norm = GaussianNB().fit(X, y)
```


```python
model_norm.classes_
```




    array([0., 1.])




```python
model_norm.class_count_
```




    array([40., 60.])




```python
model_norm.class_prior_
```




    array([0.4, 0.6])




```python
model_norm.theta_[0], model_norm.sigma_[0]
```




    (array([-1.96197643, -2.00597903]), array([1.02398854, 2.31390497]))




```python
rv0 = sp.stats.multivariate_normal(model_norm.theta_[0], model_norm.sigma_[0])
rv1 = sp.stats.multivariate_normal(model_norm.theta_[1], model_norm.sigma_[1])

xx1 = np.linspace(-5, 5, 100)
xx2 = np.linspace(-5, 5, 100)
XX1, XX2 = np.meshgrid(xx1, xx2)
plt.grid(False)
plt.contour(XX1, XX2, rv0.pdf(np.dstack([XX1, XX2])), cmap=mpl.cm.cool)
plt.contour(XX1, XX2, rv1.pdf(np.dstack([XX1, XX2])), cmap=mpl.cm.hot)
plt.scatter(X0[:, 0], X0[:, 1], marker="o", c='b', label="y=0")
plt.scatter(X1[:, 0], X1[:, 1], marker="s", c='r', label="y=1")

x_new = [0, 0]
plt.scatter(x_new[0], x_new[1], c="g", marker="x", s=150, linewidth=5)
plt.legend()
plt.title("ë‚˜ì´ë¸Œë² ì´ì¦ˆë¡œ ì¶”ì •í•œ ë°ì´í„°ì˜ í™•ë¥ ë¶„í¬")
plt.axis("equal")
plt.show()
```


![png](output_11_0.png)


### ì—°ìŠµë¬¸ì œ

ë¶“ê½ƒ ë¶„ë¥˜ë¬¸ì œë¥¼ ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì—¬ í’€ì–´ë³´ì.

(1) ê°ê°ì˜ ì¢…ì´ ì„ íƒë  ì‚¬ì „í™•ë¥ ì„ êµ¬í•˜ë¼.

(2) ê°ê°ì˜ ì¢…ì— ëŒ€í•´ ê½ƒë°›ì¹¨ì˜ ê¸¸ì´, ê½ƒë°›ì¹¨ì˜ í­, ê½ƒìì˜ ê¸¸ì´, ê½ƒìì˜ í­ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•˜ë¼.

(3) í•™ìŠµìš© ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ê³  ë‹¤ìŒì„ ê³„ì‚°í•˜ë¼.

- ë¶„ë¥˜ê²°ê³¼í‘œ 
- ë¶„ë¥˜ë³´ê³ ì„œ



```python
from sklearn.datasets import load_iris
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.naive_bayes import GaussianNB

# 1. ë°ì´í„° ë¡œë“œ

iris = load_iris()
X = np.array(iris.data)
y = np.array(iris.target)

# 2. naive-bayes í•™ìŠµ
model = GaussianNB().fit(X, y)

# 3. ì‚¬ì „í™•ë¥  êµ¬í•˜ê¸°
model.class_prior_
```




    array([0.33333333, 0.33333333, 0.33333333])




```python
# 4. ê°ê°ì˜ ì¢…(classë³„) featureì˜ í‰ê· , ë¶„ì‚°(ëª¨ìˆ˜)ë¥¼ êµ¬í•˜ë¼

print(model.theta_[0], model.sigma_[0],'\n',model.theta_[1], model.sigma_[1],'\n',model.theta_[2], model.sigma_[2])
```

    [5.006 3.428 1.462 0.246] [0.121764 0.140816 0.029556 0.010884] 
     [5.936 2.77  4.26  1.326] [0.261104 0.0965   0.2164   0.038324] 
     [6.588 2.974 5.552 2.026] [0.396256 0.101924 0.298496 0.073924]



```python
# 5. ë¶„ë¥˜ ê²°ê³¼í‘œ
y_pred = model.predict(X)
y_true = y
confusion_matrix(y_true, y_pred)
```




    array([[50,  0,  0],
           [ 0, 47,  3],
           [ 0,  3, 47]])




```python
# 6. ë¶„ë¥˜ ë¦¬í¬íŠ¸
print(classification_report(y_true, y_pred, target_names=iris.target_names))
```

                  precision    recall  f1-score   support
    
          setosa       1.00      1.00      1.00        50
      versicolor       0.94      0.94      0.94        50
       virginica       0.94      0.94      0.94        50
    
        accuracy                           0.96       150
       macro avg       0.96      0.96      0.96       150
    weighted avg       0.96      0.96      0.96       150
    


# 2. ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•


    - ë°ì´í„° : 0 ë˜ëŠ” 1ì˜ ê°’ (ì´ì§„ë¶„ë¥˜)   
        ex) íŠ¹ì • ë‹¨ì–´ê°€ ìˆê³  ì—†ê³ ê°€ ë°ì´í„°ë¡œ ì£¼ì–´ì§ˆ ë•Œ (BOW : feature: íŠ¹ì • ë‹¨ì–´(í† í°), row: ë°ì´í„°ë§ˆë‹¤ ìˆê³ (1) ì—†ê³ (0)) 
        
    - ë² ë¥´ëˆ„ì´ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• : ë² ë¥´ëˆ„ì´ ê°€ëŠ¥ë„ ëª¨í˜•ì„ ì ìš©í•œ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨ë¸
        ë² ë¥´ëˆ„ì´ ê°€ëŠ¥ë„ ëª¨í˜• : ë² ë¥´ëˆ„ì´pdfì— ì¡°ê±´ë¶€ ë…ë¦½ ì ìš©
   

**ì‹¤ìŠµ : MNIST ìˆ«ì ë¶„ë¥˜ë¬¸ì œ**

```
(1) MNIST ìˆ«ì ë¶„ë¥˜ë¬¸ì œì—ì„œ sklearn.preprocessing.Binarizerë¡œ xê°’ì„ 0, 1ë¡œ ë°”ê¾¼ë‹¤
(ê°’ì´ 8 ì´ìƒì´ë©´ 1, 8 ë¯¸ë§Œì´ë©´ 0). ì¦‰ í°ìƒ‰ê³¼ ê²€ì€ìƒ‰ í”½ì…€ë¡œë§Œ êµ¬ì„±ëœ ì´ë¯¸ì§€ë¡œ ë§Œë“ ë‹¤(ë‹¤ìŒ ì½”ë“œ ì°¸ì¡°)

from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data
y = digits.target
from sklearn.preprocessing import Binarizer
X = Binarizer(7).fit_transform(X)

ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì ìš©í•˜ì. ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë¶„ë¥˜ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë¼.
```


```python
# 1. ìˆ«ì ë°ì´í„° ë¡œë“œ + ì´ì§„í™”( binarizer ) : 'ì´ì§„í™”'ëŠ” ë² ë¥´ëˆ„ì´ ëª¨í˜•ìœ¼ë¡œ ì‚¬ìš©í•´ ë¶„ì„í•˜ê¸° ìœ„í•´ ì„ì˜ë¡œ ê°€ê³µí•œ ê²ƒ

from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data
y = digits.target
from sklearn.preprocessing import Binarizer
X = Binarizer(7).fit_transform(X)
```


```python
# 2. ë² ë¥´ëˆ„ì´ ëª¨í˜•ìœ¼ë¡œ í•™ìŠµ

from sklearn.naive_bayes import BernoulliNB
model_bern = BernoulliNB().fit(X, y)
```


```python
# 3. ë² ë¥´ëˆ„ì´ ëª¨í˜• ë¶„ë¥˜ ê²°ê³¼
# ë¶„ë¥˜ë³´ê³ ì„œ í˜•ì‹

y_pred = model_bern.predict(X)
y_true = y
print(classification_report(y_true, y_pred))
```

                  precision    recall  f1-score   support
    
               0       0.99      0.97      0.98       178
               1       0.80      0.80      0.80       182
               2       0.91      0.90      0.91       177
               3       0.93      0.85      0.89       183
               4       0.96      0.94      0.95       181
               5       0.92      0.88      0.90       182
               6       0.97      0.96      0.97       181
               7       0.91      0.99      0.95       179
               8       0.80      0.82      0.81       174
               9       0.80      0.87      0.83       180
    
        accuracy                           0.90      1797
       macro avg       0.90      0.90      0.90      1797
    weighted avg       0.90      0.90      0.90      1797
    


```
ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì ìš©í•˜ì. ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë¶„ë¥˜ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë¼.

(2) BernoulliNB í´ë˜ìŠ¤ì˜ binarize ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ë¬¸ì œë¥¼ í’€ì–´ë³¸ë‹¤.
```


```python
X2 = digits.data
model_bern2 = BernoulliNB(binarize=7).fit(X2, y)
y_pred2 = model_bern2.predict(X2)
print(classification_report(y_true, y_pred2))
```

                  precision    recall  f1-score   support
    
               0       0.99      0.97      0.98       178
               1       0.80      0.80      0.80       182
               2       0.91      0.90      0.91       177
               3       0.93      0.85      0.89       183
               4       0.96      0.94      0.95       181
               5       0.92      0.88      0.90       182
               6       0.97      0.96      0.97       181
               7       0.91      0.99      0.95       179
               8       0.80      0.82      0.81       174
               9       0.80      0.87      0.83       180
    
        accuracy                           0.90      1797
       macro avg       0.90      0.90      0.90      1797
    weighted avg       0.90      0.90      0.90      1797
    


```
(3) ê³„ì‚°ëœ ëª¨í˜•ì˜ ëª¨ìˆ˜ ë²¡í„° ê°’ì„ ê° í´ë˜ìŠ¤ë³„ë¡œ 8x8 ì´ë¯¸ì§€ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚´ë¼. ì´ ì´ë¯¸ì§€ëŠ” ë¬´ì—‡ì„ ëœ»í•˜ëŠ”ê°€?
```


```python
# 'ê³„ì‚°ëœ ëª¨í˜•ì˜ ëª¨ìˆ˜ ë²¡í„° ê°’' í™•ì¸
# 10ê°œì˜ í´ë˜ìŠ¤, 8*8 ë°ì´í„°(ê° í´ë˜ìŠ¤ ë‹¹)ë¥¼ 64*1ë¡œ í¼ì³ë†“ì€ ê²ƒ
# reshape -> 8*8 ë°ì´í„°ë¡œ ë³µì› -> ì´ë¯¸ì§€ë¡œ í™•ì¸ (ë¶„ë¥˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒ)

model_bern.feature_log_prob_.shape
```




    (10, 64)




```python
# reshape -> 8*8 ë°ì´í„°ë¡œ ë³µì›

result = model_bern.feature_log_prob_.reshape(10,8,8)
result.shape
```




    (10, 8, 8)




```python
# ì´ë¯¸ì§€ë¡œ í™•ì¸ (ë¶„ë¥˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒ)
plt.imshow(result[0],cmap=plt.cm.binary)
```




    <matplotlib.image.AxesImage at 0x12c718490>




![png](output_27_1.png)



```python
plt.imshow(result[1],cmap=plt.cm.binary)
```




    <matplotlib.image.AxesImage at 0x12bf9a790>




![png](output_28_1.png)



```python
plt.imshow(result[2],cmap=plt.cm.binary)
```




    <matplotlib.image.AxesImage at 0x12bf64bd0>




![png](output_29_1.png)


# 3. ë‹¤í•­ë¶„í¬ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•


    - ë°ì´í„° : kê°œ í´ë˜ìŠ¤
        ex) íŠ¹ì • ë‹¨ì–´ì˜ 'ë¹ˆë„ìˆ˜' ì²´í¬ ê°€ëŠ¥ (BOW : feature: íŠ¹ì • ë‹¨ì–´(í† í°), row: ë¹ˆë„ìˆ˜) 
        
    - ë‹¤í•­ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• : ë‹¤í•­ë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜•ì„ ì ìš©í•œ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨ë¸
        ë‹¤í•­ë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜• : ë‹¤í•­ë¶„í¬pdfì— ì¡°ê±´ë¶€ ë…ë¦½ ì ìš©
   

### ì—°ìŠµë¬¸ì œ

**1. MNIST ìˆ«ì ë¶„ë¥˜ë¬¸ì œë¥¼ ë‹¤í•­ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì—¬ í’€ê³  ì´ì§„í™”(Binarizing)ë¥¼ í•˜ì—¬ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì ìš©í–ˆì„ ê²½ìš°ì™€ ì„±ëŠ¥ì„ ë¹„êµí•˜ë¼.**




```python
# 1. ìˆ«ì ë°ì´í„° ë¡œë“œ : 'ì´ì§„í™”'í•˜ì§€ ì•ŠìŒ!

from sklearn.datasets import load_digits
digits = load_digits()
X = digits.data
y = digits.target
```


```python
# 2. ë‹¤í•­ë¶„í¬ ëª¨í˜•ìœ¼ë¡œ í•™ìŠµ

from sklearn.naive_bayes import MultinomialNB
model_mul = MultinomialNB().fit(X, y)
```


```python
# 3. ë‹¤í•­ë¶„í¬ ëª¨í˜• ë¶„ë¥˜ ê²°ê³¼
# ë¶„ë¥˜ë³´ê³ ì„œ í˜•ì‹
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

y_pred = model_mul.predict(X)
y_true = y
print(classification_report(y_true, y_pred))
```

                  precision    recall  f1-score   support
    
               0       0.99      0.98      0.99       178
               1       0.87      0.75      0.81       182
               2       0.90      0.90      0.90       177
               3       0.99      0.87      0.93       183
               4       0.96      0.96      0.96       181
               5       0.97      0.86      0.91       182
               6       0.98      0.97      0.98       181
               7       0.89      0.99      0.94       179
               8       0.78      0.89      0.83       174
               9       0.76      0.88      0.82       180
    
        accuracy                           0.91      1797
       macro avg       0.91      0.91      0.91      1797
    weighted avg       0.91      0.91      0.91      1797
    



```python
# 4. ì´ë¯¸ì§€ë¡œ ë‚˜íƒ€ë‚´ê¸°

model_mul.feature_log_prob_.shape
```




    (10, 64)




```python
result_mul = model_mul.feature_log_prob_.reshape(10,8,8)
```


```python
plt.imshow(result_mul[0])
```




    <matplotlib.image.AxesImage at 0x11d5bb910>




![png](output_37_1.png)



```python
plt.imshow(result_mul[1])
```




    <matplotlib.image.AxesImage at 0x11fb667d0>




![png](output_38_1.png)



```python
plt.imshow(result_mul[2])
```




    <matplotlib.image.AxesImage at 0x11fcd9e10>




![png](output_39_1.png)



```python
plt.imshow(result_mul[3])
```




    <matplotlib.image.AxesImage at 0x11fe94b50>




![png](output_40_1.png)

